{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71650b87-35d4-4142-b655-642d571edf40",
   "metadata": {},
   "source": [
    "# Logistic regression \n",
    "\n",
    "For Binary classification problems:\n",
    "\n",
    "\n",
    "Given $x$\n",
    "\n",
    "$$z = w^Tx + b$$\n",
    "\n",
    "\n",
    "$$\\hat{y}=a =\\sigma(w^Tx + b)$$\n",
    "\n",
    "Interpret,\n",
    "$$\\hat{y} = P(y=1|x)$$\n",
    "\n",
    "where, $$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "where, $x$ $\\epsilon$ $\\mathbb{R}^{n_{x}}$, $w$ $\\epsilon$ $\\mathbb{R}^{n_{x}}$ , $b$ $\\epsilon$ $\\mathbb{R}$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adde0d1-6bb1-49a9-8924-2244bd101f1a",
   "metadata": {},
   "source": [
    "# Logistic regression cost function\n",
    "\n",
    "## Loss (error) function\n",
    "\n",
    "$$L(\\hat{y},y)=-(y\\log\\hat{y} + (1-y)\\log(1-\\hat{y}))$$\n",
    "\n",
    "### Explanation for Loss function\n",
    "\n",
    "$$\\hat{y} = P(y|x)$$\n",
    "Note \n",
    "$$P(y|x) = \\hat{y}^y(1-\\hat{y})^{(1-y)}$$\n",
    "If y = 1\n",
    "$$P(y|x) = \\hat{y}$$ \n",
    "If y = 0\n",
    "$$P(y|x) = 1-\\hat{y}$$\n",
    "\n",
    "$$log(P(y|x)) = - L(\\hat{y},y)$$\n",
    "\n",
    "## Cost function on m exmaples\n",
    "\n",
    "$$ J(w,b)  = \\frac{1}{m}\\sum_{i=1}^{m}L(\\hat{y}^{(i)},y^{(i)})$$\n",
    "$$ J(w,b)  = -\\frac{1}{m}\\sum_{i=1}^{m}(y^{(i)}\\log\\hat{y}^{(i)} + (1-y^{(i)})\\log(1-\\hat{y}^{(i)}))$$\n",
    "\n",
    "We want to find $w$ and $b$ that minimizes $J(w,b)$\n",
    "\n",
    "### Explanation Cost function on m exmaples\n",
    "\n",
    "$$log p(\\rm{trainingset}) = log \\prod_{i=1}^{m}P(y^{(i)}|x^{(i)})$$\n",
    "$$log p(\\rm{trainingset}) = -  \\sum_{i=1}^{m}L(\\hat{y}^{(i)},y^{(i)})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959b0d99-8ba3-43ae-8153-f031a0aba122",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "Pertubations in w and b to reach the minima\n",
    "$$ w:= w - \\alpha \\frac{\\partial{J(w,b)}}{\\partial w} $$\n",
    "$$ b:= b - \\alpha \\frac{\\partial{J(w,b)}}{\\partial b} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f748f91-0e70-476f-bc5c-61873f5d39a1",
   "metadata": {},
   "source": [
    "### Logistic regression derivatives\n",
    "\n",
    "#### Backward propogation\n",
    "\n",
    "$$ a = \\hat{y}=\\sigma(w^Tx + b)$$\n",
    "\n",
    "1. da\n",
    "$$ da = \\frac{\\partial L(a,y) }{\\partial a} $$\n",
    "\n",
    "$$ da = \\frac{\\partial}{\\partial a} (-y\\log a - (1-y)\\log(1-a))  =-\\frac{y}{a} + \\frac{1-y}{1-a} $$\n",
    "\n",
    "2. dz\n",
    "\n",
    "$$ dz = \\frac{\\partial L(a,y) }{\\partial z} = \\frac{\\partial L(a,y) }{\\partial a} \\frac{\\partial a }{\\partial z}  = a - y$$\n",
    "\n",
    "3. dw\n",
    "\n",
    "$$ dw = \\frac{\\partial L(a,y) }{\\partial w} = xdz$$\n",
    "\n",
    "4. db\n",
    "\n",
    "$$ db = dz $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93227edd-1074-4ec4-af22-e895ef23b5ab",
   "metadata": {},
   "source": [
    "Forward Propagation:\n",
    "- You get X\n",
    "- You compute $A = \\sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})$\n",
    "- You calculate the cost function: $J = -\\frac{1}{m}\\sum_{i=1}^{m}(y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)}))$\n",
    "\n",
    "Here are the two formulas you will be using: \n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T$$\n",
    "$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
